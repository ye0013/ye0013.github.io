---
title: Pattern Recognition - 1.Introduction
description: Introduction to Pattern Recognition
categories:
 - pattern recognition
tags: pattern recognition
---

> 패턴 인식의 핵심적인 내용은 `특징`과 `분류`라는 두 가지로 구성된다.

## 패턴 인식 처리과정
패턴 -> `특징` -> `분류` -> 부류

이제부터 박보검 얼굴을 인식하는 모델을 만듭시다. 우리는 사진에서 쳐진 눈, 날렵한 코, 갸름한 얼굴형 등을 보고 박보검이라 판단할 수 있겠죠. 이 때에는 눈, 코, 입 외에도 여러가지 `특징` <sup>`feature`</sup> 을 사용할 수 있습니다.


그럼 사진을 보았을 때 "박보검은 눈이 쳐지고, 얼굴이 갸름하다는 패턴이 있는데 이 사진의 사람을 눈이 찢어졌고 얼굴이 퉁퉁하네? 그럼 박보검이 아니네" 와 같은 판단을 할 수 있겠죠. 이렇게 특징들에 주목하고, 각 특징이 갖는 값의  패턴을 이미 알고 있으니 의사 결정, 즉 `분류` <sup>`classification`</sup> 를 할 수 있습니다.


이제 이 모델을 더 확장해서 여러 사람의 얼굴은 인식하는 모델을 만들어 봅시다. 만약 10명의 얼굴을 구별하여 인식하는 모델을 만들 면 사람0, 사람1, ..., 사람9까지 총 열 종류의 패턴이 발생합니다. 열 종류를 각각 `부류` <sup>`class`</sup> 라고 합니다. 그리고 흔히 부류의 개수를 *`M`* 이란 변수로 표시하고, 부류는 w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>M</sub> 으로 표시합니다. 이 모델의 경우 *M*=10이며, 사람0은 w<sub>1</sub>, 사람1은 w<sub>2</sub>, 이런 식으로 사람9는 w<sub>10</sub>이 됩니다.


만일 기사 사진으로 모델을 학습시킨다면 기사 사진이 찍힐 때마다 수많은 패턴이 발생하겠죠. 이렇게 패턴을 발생시키는 도메인을 `패턴 원천` <sup>`pattern source`</sup> 이라고 부릅니다.


**Summary**
- Feature
- Classification
- Class


### 데이터베이스 수집 (Sample)
은행에서 필기 숫자를 인식하는 시스템을 만든다고 가정해봅시다. 우선 패턴 원천 즉, 고객이 작성한 실제 은행 전표에서 패턴을 수집해야겠죠. 이렇게 인식 시스템을 만들이 귀해 수집한 패턴을 `샘플` <sup>`sample`</sup> 이라고 합니다.


보통 이렇게 수집한 샘플은 두 개의 집합으로 나누어 놓습니다. 이 중에서 인식기를 만드는데 사용할 집합을 `훈련 집합` <sup>`training set`</sup> 이라고 하고, 인식기를 만든 후에 성능을 평가하기 위해 사용하는 집합은 `테스트 집합` <sup>`test set`</sup> 이라고 합니다.


데이터베이스는 양적으로 충분히 커야 하며 질적으로 충분히 다양한 스타일의 패턴을 담고 있어햐 합니다.

**Summary**
- Sample

### 특징 (Feature)
이제 샘플을 분류하는데 사용할 `특징` <sup>`feature`</sup> 을 추출해야 합니다. *M* 개의 클래스가 서로 다른 값을 보이는 특징일수록 분류하기 유리하기 때문에 좋은 특징이라 할 수 있습니다.

#### 어떤 특징을 사용해야 할까?
개별적인 화소를 특징으로 하면 어떨까요? 8X8 픽셀 이미지를 사용한다고 했을 때, 특징은 64개가 됩니다. 이들을 벡터로 표현하면 64 차원 벡터 ***x*** = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>64</sub>)<sup>T</sup> 가 된다. 이때 **`x`** 를 `특징 벡터` <sup>`feature vector`</sup> 라고 한다. 그리고 흔히 특징의 개수를 *`d`* 로 표기한다.

또 다른 특징을 생각해봅시다. 비트맵을 이등분하여 상단과 하단의 검은 화소이 개수를 세고 이들의 비를 첫 번째 특징으로 합시다. 이번에는 비트맵의 좌단과 우단의 비를 두 번째 비로 합시다. 이렇게 하면 2 차원 특징 벡터  ***x*** = (x<sub>1</sub>, x<sub>2</sub>)<sup>T</sup> 를 얻을 수 있습니다. 이들은 `특징 공간` <sup>`feature space`</sup>에 그려보면 아래와 같습니다.


#### 이 특징들은 서로 다른 클래스를 얼마나 잘 구별해 줄까?
우선 개별 화소를 특징으로 삼는 방법에 대해 생각해보자. 첫 번째 특징 x<sub>1</sub>은 거의 모든 샘플이 0의 값을 갖는다. 따라서 전혀 쓸모 없는 특징이다. 그럼 화소 개수의 비를 특징으로 삼는 방법은 어떨까? 완벽하지 않지만 어느 정도 구별해 주는 능력이 있어 쓸모는 있다. 이와 같이 특징이 서로 다른 클래스를 얼마나 잘 구별해 주느냐와 관련한 척도를 특징의 `분별력` <sup>`discriminating power`</sup> 이라고 한다.


#### 특징은 몇 개를 사용해야 할까?
특징을 많이 사용한다고 해서 좋은게 아니다. 쓸모 없는 특징이 포함될 수 있고, 두 개의 특징이 개별적으로는 모두 쓸모가 있지만 상관 관계가 강하여 하나만 있어도 되는 경우가 있다. 또한 특징 벡터의 차원이 커질수록 계산량이 기하급수적으로 커지는 경우가 많다. 특징 벡터의 차원에 따라 계산량 혹은 메모리 요구량이 폭발적으로 증가하는 현상을 `차원의 저주` <sup>`curse of dimensionality`</sup> 라고 한다.

**Summary**
- Feature
- Feature vector
- Feature space
- Discriminating power
- Curse of dimensionality




### 분류 (Classification)
어떤 패턴이 들어왔을 때 이것을 *M* 개의 클래스 중 하나로 할당해 주는 작업을 `분류` <sup>`classification`</sup> 이라고 하고 이 작업을 담당하는 프로그램을 `분류기` <sup>`classifier`</sup> 라고 한다.

#### 어떻게 분류할 것인가?
두 개의 클래스 w<sub>1</sub>, w<sub>2</sub>가 있고, 특징 공간은 3차원이라고 합시다. w<sub>1</sub> 과 w<sub>2</sub> 는 각각 10개과 12개의 훈련 샘플을 갖고 있습니다.

가장 쉽게 생각할 수 있는 방법은 직선으로 두 클래스를 구별하는 것입니다. 혹은 직선 외에도 2차 곡선 혹은 그 이상의 차수의 곡선 등을 사용할 수 있겠죠. 이렇게 `모델 선택` <sup>`model selection`</sup> 은 분류기를 표현하는 수학적 모델로 무엇을 쓸 것인지 결정하는 작업입니다.


